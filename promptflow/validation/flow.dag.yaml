$schema: https://azuremlschemas.azureedge.net/promptflow/latest/Flow.schema.json
environment:
  python_requirements_txt: requirements.txt
additional_includes:
- ../../src/
- ../prompts/
inputs:
  deployment_name:
    type: string
    default: gpt-4o-2024-05-13
  chat_history:
    type: list
    default: []
    is_chat_history: true
  question:
    type: string
    default: what is BERT?
    is_chat_input: true
outputs:
  answer:
    type: string
    reference: ${generate_response.output}
    is_chat_output: true
  groundedness_score:
    type: int
    reference: ${parse_groundedness.output}
    is_chat_output: false
nodes:
- name: rewrite_question
  type: llm
  source:
    type: code
    path: prompts/rewrite_question_prompt.jinja2
  inputs:
    deployment_name: ${inputs.deployment_name}
    temperature: 1
    top_p: 1
    max_tokens: 16
    response_format:
      type: text
    chat_history: ${inputs.chat_history}
    question: ${inputs.question}
  provider: AzureOpenAI
  connection: aoai
  api: chat
  module: promptflow.tools.aoai
  use_variants: false
- name: generate_response
  type: llm
  source:
    type: code
    path: generate_response_prompt.jinja2
  inputs:
    deployment_name: ${inputs.deployment_name}
    temperature: 0
    top_p: 1
    max_tokens: 1000
    response_format:
      type: text
    presence_penalty: 0
    frequency_penalty: 0
    question: ${rewrite_question.output}
    context: ${rewrite_question.output}
    chat_history: ${inputs.chat_history}
  provider: AzureOpenAI
  connection: aoai
  api: chat
  module: promptflow.tools.aoai
  use_variants: false
- name: groundedness
  type: llm
  source:
    type: code
    path: prompts/metrics/gpt_groundedness_prompt.jinja2
  inputs:
    question: ${rewrite_question.output}
    context: ${generate_response.output}
    answer: ${generate_response.output}
    deployment_name: gpt-4o-2024-05-13
    response_format:
      type: text
  connection: aoai
  api: chat
- name: parse_groundedness
  type: python
  source:
    type: code
    path: src/tools/metrics/parse_llm_evaluation_score.py
  inputs:
    llm_evaluation_score: ${groundedness.output}
