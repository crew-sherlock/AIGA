# AI Evaluation

The purpose of the evaluation framework is two fold:

- Ensure that the output generated by the AI is meeting the business expectations
- Comply with regulatory necessity to validate and monitor the AI systems. More information concerning the AI standards at GSK (including the AI SOP and the accountability report template) can be found on [the AI responsible Hub](https://myconnect.gsk.com/sites/modern-gskdigitalandtech#/sitepages/responsible-ai-hub.aspx). Request to raise a GSC review can be done in [SNOW](https://servicenow.gsk.com/home?id=sc_cat_item&table=sc_cat_item&sys_id=b546f6801bbec2d04ab887b8e34bcb16).

As part of this article, we explain the evaluation strategy set in place to meet both above requirements. This content can be used to fill in the Accountability Report and follows the structure of the summary sheet.

## Success criteria

At the beginning of each project, it is very important to agree on the success criteria for the project. This will determine the validation metrics and strategy to put in place.

In general, the business needs can usually be met by ensuring that the AI output is :

- Grounded: into the context provided (no hallucination or out of context information) - the context in this case refers to the data provided to the AI system (unstructured and structured sources and instructions)
- Complete (relevant): ie encompassing all the information mentioned in the context (data sources) to answer the question (not omitting any relevant information)
- Properly formatted (accuracy): following GSK code of conduct (non violent, adequate language (based on the persona), grammatically correct, concise, ethical, etc).
- The retrieved context is correct and complete

Given the unbounded and unstructured nature of Generative AI, there does not exist quantitative metrics capable to technically translate the above success criteria. Even though blue and rouge score exists and are quantitative metrics, they are way too narrow for the purpose of evaluating our complex application and are never used in practice (outside the realm of GenAI model development).

The agreed way to evaluate the GenAI applications is to use another LLM to score the output of the first one. It is proven that evaluating is a much easier task than generating hence not creating an inception loops of LLMs. You can even use a smaller model to evaluate the output from bigger models.

We have standard metrics for evaluating the 3 main criteria (TO BE CREATED - WE HAVE A STORY)

## Training data used

Usually, it is more efficient to put in place RAG systems rather than fine tune the models themselves. So usually, besides for few shot learning, no data is used to train the model per se and we rely on commercial provider to train the Generative AI models.

## Validation process

The validation process ensures that the quality of the AI system is under control and that the system operates as intended. In other words, it ensures that the success criteria are met.

To fine tune the AI system and determine the score and the metrics, we will go through an iterative and thorough validation of the metrics, the scores and the generated AI output by means of expert scoring and validation.

The process of developing a GenAI products goes like:

![cycle](./assets/Lifecycle.svg)

- Once the business requirements (including the success criteria) have been captured, we need to collect a sample of data.
- The basic document processing pipelines, prompts and flows can be built. Refer to [running promptflow](/docs/onboarding/running-promptflow.md)
- Based on the payload from the flows, we create a test set (identified by the business as candidates for the golden dataset). The test set is a xxx file
- With the test set, we run the pipeline and flows in order to generate the output
- The metrics defined by the business are included in the inference flow and the generated AI output is evaluated against these. Check [how to add a metric](/docs/onboarding/adding-metrics.md)
- If the metrics do not meet expectations, we will optimise the pipelines and flows
- If the metrics do meet expectations, we will collect feedback from the SME's, using an excel file
- With the feedback from the business, we will go back to optimising the pipelines, flows and prompts
- Once we have reached satisfactory level, we consider the validated output from business the Golden Dataset
- Before deploying a flow, the offline evaluation flow (comparing the AI generated output and the golden dataset) will always run
- Once the metrics and flows have been logged and validated, the flows and pipelines can be deployed with our [CD pipelines](/docs/onboarding/deployment.md)
- While running as usual, the online metrics are [captured into App Insights](/docs/onboarding/observability.md) and [metrics and alerts](/docs/onboarding/alerting.md) are implemented
- If an alert is triggered or if an incident is raised by the users, the Root Cause Analysis will start
- We will go back to optimising the pipelines and flows, and straight to the production flow, skipping the evaluation flow since we do already have a Golden Dataset

In practice, multiple rounds are necessary between Experimentation and Evaluation, to refine the metrics and fine tune the flows and pipelines to get the desired output. Also usually, the Golden Dataset is not built in one go, it will be iteratively built, as more features are added (for example translation could be added).

## Monitoring & Maintenance

In order to ensure that the AI system operates as intended, we implement monitoring and maintenance measures.

- The metrics that we have implemented to evaluate the systems are calculated for each answer generated. These metrics are used in the system to warn the user of the quality of the output: if the metrics are not >=4, the user should be warned of the poor quality of the output or no output should be given to the user.
- Furthermore, these metrics are logged into App Insights and AppInsights metrics are calculated over a rolling window of 100 answers generated (App Insights Metrics - AIM). Based on the AIM rolling windows, we do have alerts if they go below the threshold of 4. These alerts are sent to L3 support GenAI that then investigates and proactively takes action to resolve the content drift.
- Once a change is implemented, to ensure robustness of the new changes, we evaluate them over our Golden Dataset (this happens automatically in our CI pipeline). The change is then deployed to production ONLY if the evaluation of the change on the Golden Dataset has metrics  >=4. There is a human in the loop validating that the metrics are meeting expectations and then releasing the new code in production (CD pipeline  is implemented for automatic deployment).

## AI Risks and Mitigations

Below is a list of the main risks with GenAI applications and their mitigation strategies.

|Risk|Explanation|Mitigation(s)|
|-------|-------------|---------------|
| Model Migration | The models are quickly decommissioned since this is a fairly new landscape and models are being released often. This risk considers only the case of redeployment of the same code, with a new model. It does not cover the implementation of new features as this follows the entire validation process again. | The golden dataset collected serve to be able to run the offline evaluation and assess whether the tool can be deployed robustly. Indeed the evaluation is integrated in the CI pipeline so it should be quick to validate a new model release and make the appropriate changes |
|Usage of GenAI embedded into third party supplier| The usage of Genie space does not allow us to control their release lifecycle. This might have an impact on the performance of the copilot. |Action for Virginie to check this with Databricks |
|Content / Data drift| The content of the data sources might drift (change over time) and the answer from the AI model would no longer be accurate | The metrics in App Insight will allow to detect this type of drift. Once this happens, the L3 support will investigate for root cause and take action in collaboration with the business stakeholders.|
| Human input|The system is not supposed to handle CSI data, nor PII data and this will be mentioned into the interface. Nevertheless, given that this is a chatbot, we can not control what the users input into the tool. The human could also input violent or inappropriate content. | We will implement content safety filters, to detect cases of violence, or inappropriate usage of the tool. The tool will then be prompted to refuse to answer to the question. These will be logged and abusive usage of the tool will be monitored and could be sanctioned |
| Model Maturity |An answer is generated however the content is incorrect due to AI inaccurate output (i.e. non English input)| The human will be warned to use critical thinking and not to take the output of the AI for granted. The Human is end responsible to use the output generated by the AI. Even though we thoroughly test the system, we can never guarantee a 100% accuracy. A feedback button will be implemented to allow users to feedback improvements into the system |
|Data access | The model could output data for which the user doesn't have access| We need to follow an identification chain throughout the backend until the data layer, to ensure that only data that the users are supposed to access are forwarded to the LLM and that the answer to the question from the user is only grounded into the right data|
