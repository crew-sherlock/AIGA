name: evaluation-workflow

on:
  workflow_call:
    inputs:
      environment:
        description: "The environment to run the workflow with (playground, dev, test, prod)"
        type: string
        required: true
    secrets:
      AZURE_CREDENTIALS:
        description: "service principal authentication to Azure"
        required: true

jobs:
  flow-experiment-and-evaluation:
    name: prompt flow experiment and evaluation job
    runs-on: ubuntu-latest
    environment:
      name: ${{ inputs.environment }}

    steps:
      - name: Checkout Actions
        uses: actions/checkout@v4

      - name: Azure login
        uses: azure/login@v2.1.1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}

      - name: Load Secrets from Azure Key Vault
        uses: ./.github/actions/load-secrets
        with:
          azureKeyVaultName: ${{ vars.KEY_VAULT_NAME }}

      - name: Set up Python Env
        uses: ./.github/actions/setup-environment

      #=====================================
      # Reads run_id.txt file. Assigns it to variable RUN_NAME
      # RUN_NAME Used in next step for evaluation of flows
      #=====================================
      - name: Download run_id.txt artifact
        uses: actions/download-artifact@v4
        with:
          name: run-id-artifact

      - name: Read PromptFlow Runs
        shell: bash
        run: |
          readarray arr <"run_id.txt"
          run_name=${arr[0]}
          echo $run_name
          echo "RUN_NAME=${run_name}"  >> "$GITHUB_ENV"
          echo $PWD

      #=====================================
      # Executes all Evaluation flows available for a scenario
      # Generates Reports for each RUN as well as consolidated one
      # Uses each RUN ID as input to run evaluation against
      # Loads appropriate evaluation data from Azure ML data asset
      # Reads appropriate field values from experiment.yaml or experiment.<env>.yaml
      # used automatic (serverless) runtime by default
      #=====================================
      - name: Execute bulk run evaluations
        uses: ./.github/actions/execute-script
        with:
          step_name: "Execute bulk run evaluations"
          script_parameter: |
            python -m llmops.common.prompt_eval \
            --subscription_id ${{ env.AML_AZURE_SUBSCRIPTION_ID }} \
            --build_id ${{ github.run_id }} \
            --base_path ${{ vars.PROMPTFLOW_BASE_PATH }} \
            --env_name ${{ inputs.environment }} \
            --run_id "$RUN_NAME"

      #=====================================
      # Published generated reports in csv and html format
      # Available as pipeline artifacts
      #=====================================
      - name: Archive CSV
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-reports
          path: ./reports
